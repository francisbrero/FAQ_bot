{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /usr/local/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /usr/local/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: llama-cpp-python in /usr/local/lib/python3.11/site-packages (0.2.11)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/site-packages (from llama-cpp-python) (4.7.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/site-packages (from llama-cpp-python) (1.25.1)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.11/site-packages (from llama-cpp-python) (5.6.3)\n",
      "\u001b[33mWARNING: Skipping /usr/local/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /usr/local/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U llama-cpp-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Model path does not exist: ~/Documents/llama/llama-2-7b-chat/ggml-model-f16_q4_0.gguf",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/francis/Documents/MadKudu/FAQ_bot/lib/test_llama.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/francis/Documents/MadKudu/FAQ_bot/lib/test_llama.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m~/Documents/llama/llama-2-7b-chat/ggml-model-f16_q4_0.gguf\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/francis/Documents/MadKudu/FAQ_bot/lib/test_llama.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m Llama(model_path \u001b[39m=\u001b[39;49m model_path,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/francis/Documents/MadKudu/FAQ_bot/lib/test_llama.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m               n_ctx \u001b[39m=\u001b[39;49m \u001b[39m2048\u001b[39;49m,            \u001b[39m# context window size\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/francis/Documents/MadKudu/FAQ_bot/lib/test_llama.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m               n_gpu_layers \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m,        \u001b[39m# enable GPU\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/francis/Documents/MadKudu/FAQ_bot/lib/test_llama.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m               use_mlock \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)        \u001b[39m# enable memory lock so not swap\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/francis/Documents/MadKudu/FAQ_bot/lib/test_llama.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/francis/Documents/MadKudu/FAQ_bot/lib/test_llama.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m[INST]<<SYS>>\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/francis/Documents/MadKudu/FAQ_bot/lib/test_llama.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt know the answer to a question, please don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt share false information.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/francis/Documents/MadKudu/FAQ_bot/lib/test_llama.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m[/INST]\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/francis/Documents/MadKudu/FAQ_bot/lib/test_llama.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m\"\"\"\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/llama_cpp/llama.py:354\u001b[0m, in \u001b[0;36mLlama.__init__\u001b[0;34m(self, model_path, n_gpu_layers, main_gpu, tensor_split, vocab_only, use_mmap, use_mlock, seed, n_ctx, n_batch, n_threads, n_threads_batch, rope_freq_base, rope_freq_scale, mul_mat_q, f16_kv, logits_all, embedding, last_n_tokens_size, lora_base, lora_scale, lora_path, numa, chat_format, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlora_path \u001b[39m=\u001b[39m lora_path\n\u001b[1;32m    353\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(model_path):\n\u001b[0;32m--> 354\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel path does not exist: \u001b[39m\u001b[39m{\u001b[39;00mmodel_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    356\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n\u001b[1;32m    357\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m llama_cpp\u001b[39m.\u001b[39mllama_load_model_from_file(\n\u001b[1;32m    358\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_path\u001b[39m.\u001b[39mencode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_params\n\u001b[1;32m    359\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Model path does not exist: ~/Documents/llama/llama-2-7b-chat/ggml-model-f16_q4_0.gguf"
     ]
    }
   ],
   "source": [
    "model_path = \"/Users/francis/Documents/llama/llama-2-7b-chat/ggml-model-f16_q4_0.gguf\"\n",
    "model = Llama(model_path = model_path,\n",
    "              n_ctx = 2048,            # context window size\n",
    "              n_gqa=8, # add this\n",
    "              n_gpu_layers = 1,        # enable GPU\n",
    "              use_mlock = True)        # enable memory lock so not swap\n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "[INST]<<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "\n",
    "What is the best way to learn programming?\n",
    "[/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "output = model(prompt = prompt, max_tokens = 120, temperature = 0.2)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
